{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "364bd179",
      "metadata": {
        "id": "364bd179"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/sample_data/heart_disease_uci.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "columns_with_missing_values = ['trestbps', 'chol', 'thalch', 'oldpeak','fbs', 'restecg', 'ca', 'slope', 'thal']\n",
        "\n",
        "df.dropna(subset=columns_with_missing_values, inplace=True)\n",
        "\n",
        "new_file_path = '/content/sample_data/missing.csv'\n",
        "df.to_csv(new_file_path, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c25db6c",
      "metadata": {
        "id": "7c25db6c"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pandas as pd\n",
        "file_path = '/content/sample_data/missing.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "label_encoder = LabelEncoder()\n",
        "columns=['sex','cp','fbs','restecg','exang','slope','thal']\n",
        "for column in columns:\n",
        "    df[column] = label_encoder.fit_transform(df[column])\n",
        "\n",
        "new_file_path = '/content/sample_data/encoded.csv'\n",
        "df.to_csv(new_file_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ea177aa",
      "metadata": {
        "id": "9ea177aa"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import pandas as pd\n",
        "file_path = '/content/sample_data/encoded.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "columns_tonormalize = ['age','cp','trestbps', 'chol', 'restecg', 'thalch', 'oldpeak', 'slope', 'ca', 'thal']\n",
        "scaler = MinMaxScaler()\n",
        "df[columns_tonormalize] = scaler.fit_transform(df[columns_tonormalize])\n",
        "\n",
        "new_file_path = '/content/sample_data/normalized.csv'\n",
        "df.to_csv(new_file_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "629182e0",
      "metadata": {
        "id": "629182e0"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "import pandas as pd\n",
        "file_path = '/content/sample_data/normalized.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "isolation_forest = IsolationForest(contamination=0.1, random_state=42)\n",
        "outliers = isolation_forest.fit_predict(df)\n",
        "\n",
        "df_filtered = df[outliers != -1]\n",
        "\n",
        "new_file_path = '/content/sample_data/outlier.csv'\n",
        "df_filtered.to_csv(new_file_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd5450f1",
      "metadata": {
        "id": "fd5450f1",
        "outputId": "72b76b08-f02d-4814-ac92-0079c1d0b1e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imbalanced-learn in c:\\users\\thasneem.desktop-met98bf\\anaconda3\\lib\\site-packages (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\thasneem.desktop-met98bf\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.24.3)\n",
            "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\thasneem.desktop-met98bf\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.11.1)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in c:\\users\\thasneem.desktop-met98bf\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.4.1.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in c:\\users\\thasneem.desktop-met98bf\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\thasneem.desktop-met98bf\\anaconda3\\lib\\site-packages (from scikit-learn>=0.24->imbalanced-learn) (2.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install imbalanced-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc37e505",
      "metadata": {
        "id": "dc37e505"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from imblearn.over_sampling import SMOTE\n",
        "file_path = '/content/sample_data/outlier.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "X = df.drop('num', axis=1)\n",
        "y = df['num']\n",
        "\n",
        "\n",
        "smote = SMOTE()\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "\n",
        "resampled_df = pd.DataFrame(X_resampled, columns=X.columns)\n",
        "resampled_df['num'] = y_resampled\n",
        "resampled_df = resampled_df.sample(frac=1)\n",
        "new_file_path = '/content/sample_data/smote.csv'\n",
        "resampled_df.to_csv(new_file_path, index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed3896f7",
      "metadata": {
        "id": "ed3896f7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dab23101",
      "metadata": {
        "id": "dab23101"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import IsolationForest\n",
        "import pandas as pd\n",
        "file_path = '/content/sample_data/smote.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "isolation_forest = IsolationForest(contamination=0.1, random_state=42)\n",
        "outliers = isolation_forest.fit_predict(df)\n",
        "\n",
        "df_filtered = df[outliers != -1]\n",
        "\n",
        "new_file_path = '/content/sample_data/outlier.csv'\n",
        "df_filtered.to_csv(new_file_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a41271f8",
      "metadata": {
        "id": "a41271f8",
        "outputId": "cf7c536d-5c1e-4b65-b8a3-57717506e54d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Target correlation\n",
            "[('chol', -0.013593902300911086), ('thalch', -0.021985098843714235), ('age', -0.04193520158417837), ('oldpeak', -0.05294047079278687), ('trestbps', -0.053183258567897254), ('ca', -0.21501819608645567), ('thal', -0.4901766309415492), ('cp', -0.5767915480527931), ('restecg', -0.6593061012316944), ('slope', -0.6795691295407513), ('exang', -1.2827238980468072), ('sex', -1.735960382385049), ('fbs', -1.9835528936757367)]\n",
            "Overlapping of features\n",
            "[('chol', 'fbs', -0.0006249783293560339), ('thalch', 'fbs', -0.00103305780782925), ('age', 'fbs', -0.001050643189835077), ('trestbps', 'fbs', -0.0012955393460958053), ('oldpeak', 'fbs', -0.0016603698369147498), ('chol', 'sex', -0.0022034778988785134), ('ca', 'fbs', -0.004293183637767436), ('thalch', 'sex', -0.005484189219011797), ('age', 'sex', -0.006693012022777328), ('thal', 'fbs', -0.007041948734600569), ('chol', 'thal', -0.00896177533245242), ('chol', 'restecg', -0.009180538900846729), ('chol', 'exang', -0.009489248362598532), ('cp', 'fbs', -0.009642223841525892), ('chol', 'slope', -0.01004616860750491), ('trestbps', 'sex', -0.010062684562831119), ('restecg', 'fbs', -0.010413041777448813), ('chol', 'ca', -0.010659362145214362), ('chol', 'cp', -0.011507588802935865), ('thalch', 'exang', -0.01228172788697058), ('oldpeak', 'slope', -0.012328499240325305), ('slope', 'fbs', -0.012378408456372168), ('thalch', 'slope', -0.012865147975253661), ('oldpeak', 'sex', -0.01351707726420152), ('chol', 'trestbps', -0.013858166931135285), ('chol', 'oldpeak', -0.014410300833377607), ('chol', 'thalch', -0.014519543734882167), ('chol', 'age', -0.014959721067124239), ('thalch', 'cp', -0.015807914462678544), ('thalch', 'ca', -0.017587150819936094), ('thalch', 'restecg', -0.017602800915402134), ('thalch', 'thal', -0.024071166919473752), ('age', 'thal', -0.024802768110807796), ('age', 'restecg', -0.025723094330031407), ('age', 'slope', -0.02625496831402502), ('age', 'ca', -0.027936835098135767), ('exang', 'fbs', -0.028047433735576086), ('oldpeak', 'exang', -0.02874679493748934), ('age', 'exang', -0.029207512691450173), ('thalch', 'oldpeak', -0.030261939403217097), ('age', 'cp', -0.03130969799350092), ('trestbps', 'thal', -0.03176587353532131), ('thalch', 'chol', -0.03310948619991264), ('thalch', 'trestbps', -0.033155104823622404), ('trestbps', 'exang', -0.03369802476808179), ('trestbps', 'slope', -0.03370457567600535), ('thalch', 'age', -0.034024257463630324), ('trestbps', 'restecg', -0.0341358919852453), ('oldpeak', 'ca', -0.03597273261898818), ('trestbps', 'ca', -0.037922366700036535), ('trestbps', 'cp', -0.04048162968804275), ('age', 'oldpeak', -0.04282337227529927), ('oldpeak', 'thal', -0.04656390691355804), ('age', 'trestbps', -0.04709976123902423), ('oldpeak', 'restecg', -0.04824288406675478), ('age', 'chol', -0.05045535831574296), ('age', 'thalch', -0.05093274156313602), ('ca', 'sex', -0.05201057501007324), ('oldpeak', 'cp', -0.05947270693168226), ('trestbps', 'oldpeak', -0.06368961042310574), ('trestbps', 'age', -0.0728076850096511), ('trestbps', 'chol', -0.07504776877971928), ('trestbps', 'thalch', -0.07576688205040151), ('oldpeak', 'trestbps', -0.09710130502390123), ('thal', 'sex', -0.10023509383442822), ('oldpeak', 'age', -0.10193029824268919), ('oldpeak', 'thalch', -0.106258361602194), ('oldpeak', 'chol', -0.10896327011867153), ('sex', 'fbs', -0.10931198646392061), ('cp', 'sex', -0.12177859224073087), ('ca', 'exang', -0.12993350433805217), ('restecg', 'sex', -0.15829682497468023), ('slope', 'sex', -0.17296794934335702), ('ca', 'slope', -0.18982321805153296), ('ca', 'restecg', -0.22870311305704266), ('ca', 'cp', -0.2597254366558486), ('ca', 'thal', -0.28961146564201823), ('thal', 'exang', -0.29676002949431696), ('cp', 'exang', -0.3129631952915026), ('ca', 'oldpeak', -0.4149824165577241), ('exang', 'sex', -0.41678118157810634), ('restecg', 'exang', -0.42118518000318844), ('ca', 'trestbps', -0.44904220603799444), ('slope', 'exang', -0.4524970638846342), ('ca', 'age', -0.4636134276466485), ('ca', 'thalch', -0.46934025289606074), ('ca', 'chol', -0.4800169828313659), ('thal', 'slope', -0.5571731236090051), ('thal', 'cp', -0.5601332852081407), ('restecg', 'slope', -0.5944449116874065), ('cp', 'slope', -0.6264240292779224), ('thal', 'restecg', -0.6421329315707702), ('cp', 'restecg', -0.6955402808535172), ('cp', 'thal', -0.7008704878459184), ('slope', 'restecg', -0.743754982101294), ('thal', 'ca', -0.7693550716274338), ('restecg', 'cp', -0.8054836507288247), ('thal', 'oldpeak', -0.8679782723566765), ('cp', 'ca', -0.8699067248392477), ('thal', 'trestbps', -0.880766758957879), ('slope', 'cp', -0.8846178645109735), ('restecg', 'thal', -0.8918475379212665), ('thal', 'age', -0.8947646195774507), ('thal', 'chol', -0.9074827160128532), ('thal', 'thalch', -0.9076920449961591), ('restecg', 'ca', -0.9369495249472716), ('slope', 'thal', -0.9525127502229063), ('cp', 'oldpeak', -0.9945983127430853), ('cp', 'trestbps', -1.001512288116323), ('cp', 'thalch', -1.0092793818869803), ('cp', 'age', -1.0119910764284876), ('cp', 'chol', -1.0189176410563192), ('slope', 'ca', -1.0252923361037787), ('restecg', 'oldpeak', -1.0671238525110223), ('restecg', 'trestbps', -1.0775484516452596), ('restecg', 'age', -1.0877218221124425), ('restecg', 'thalch', -1.091688231542645), ('restecg', 'chol', -1.0964285908753635), ('slope', 'oldpeak', -1.1377183757529221), ('slope', 'trestbps', -1.1817145138862142), ('slope', 'thalch', -1.1891018578647399), ('slope', 'age', -1.1913762879060286), ('slope', 'chol', -1.1983767604570532), ('exang', 'slope', -1.2814391344098734), ('exang', 'cp', -1.3448841742695914), ('exang', 'restecg', -1.3700763970489593), ('exang', 'thal', -1.428333693128164), ('sex', 'exang', -1.5483796451677188), ('exang', 'ca', -1.5487009016527382), ('exang', 'oldpeak', -1.6134849217804907), ('exang', 'thalch', -1.6244753721807828), ('exang', 'chol', -1.6281448206718256), ('exang', 'trestbps', -1.6307079123906858), ('exang', 'age', -1.635439040599863), ('sex', 'slope', -1.7610836177236298), ('sex', 'cp', -1.7964199127441085), ('sex', 'restecg', -1.7984911618659507), ('sex', 'thal', -1.8146013020964336), ('sex', 'ca', -1.8719211857775413), ('sex', 'chol', -1.8886304562879592), ('sex', 'oldpeak', -1.889023319445945), ('sex', 'age', -1.889136421828054), ('fbs', 'sex', -1.8892959177340838), ('sex', 'trestbps', -1.8895368533569812), ('sex', 'thalch', -1.8898416550928043), ('fbs', 'exang', -1.9645106601751323), ('fbs', 'slope', -1.985829834891108), ('fbs', 'restecg', -1.9875319654906627), ('fbs', 'cp', -1.989133461990001), ('fbs', 'thal', -1.9892177612221758), ('fbs', 'ca', -1.9933468356774613), ('fbs', 'chol', -1.993832769583354), ('fbs', 'age', -1.9940134162861505), ('fbs', 'trestbps', -1.9940753750336158), ('fbs', 'thalch', -1.9941136181077959), ('fbs', 'oldpeak', -1.994195326907544)]\n",
            "Columns selected by SU feature selection method:\n",
            "['age', 'sex', 'cp', 'trestbps', 'chol', 'restecg', 'thalch', 'exang', 'oldpeak', 'slope', 'ca', 'thal', 'num']\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from math import log2\n",
        "\n",
        "def calculate_entropy(data):\n",
        "\n",
        "    value_counts = data.value_counts(normalize=True)\n",
        "    entropy = -sum(p * log2(p) for p in value_counts)\n",
        "    return entropy\n",
        "\n",
        "def calculate_joint_entropy(x, y):\n",
        "\n",
        "    joint_data = pd.concat([x, y], axis=1)\n",
        "    joint_entropy = calculate_entropy(joint_data)\n",
        "    return joint_entropy\n",
        "\n",
        "def calculate_su(x, y):\n",
        "\n",
        "    entropy_x = calculate_entropy(x)\n",
        "    entropy_y = calculate_entropy(y)\n",
        "    joint_entropy_xy = calculate_joint_entropy(x, y)\n",
        "    su = 2 * (entropy_x - joint_entropy_xy) / (entropy_x + entropy_y)\n",
        "    return su\n",
        "\n",
        "def feature_selection_by_su(data, target_column, threshold=0.7):\n",
        "\n",
        "    su_values = {} #  su between feature and target\n",
        "    for column in data.columns:\n",
        "        if column != target_column:\n",
        "            su_values[column] = calculate_su(data[column], data[target_column])\n",
        "\n",
        "    selected_columns = []\n",
        "#    to remove less correlated features\n",
        "    sorted_su_values = sorted(su_values.items(), key=lambda x: x[1], reverse=True)\n",
        "    last_item_su_value = sorted_su_values[-1][1]\n",
        "    if -0.02 <= last_item_su_value <= 0:\n",
        "        column_to_drop = sorted_su_values[-1][0]\n",
        "        data.drop(columns=[column_to_drop], inplace=True)\n",
        "\n",
        "#    su between feature to feature\n",
        "    su_values = {}\n",
        "    for column in data.columns:\n",
        "        if column != target_column:\n",
        "            su_values[column] = []\n",
        "            for other_column in data.columns:\n",
        "                if other_column != column and other_column != target_column:\n",
        "                    su_value = calculate_su(data[column], data[other_column])\n",
        "                    su_values[column].append((other_column, su_value))\n",
        "\n",
        "    print('Target correlation')\n",
        "    print(sorted_su_values)\n",
        "#    for redundant features removal\n",
        "    tosort = []\n",
        "    highly_correlated_features = []\n",
        "\n",
        "    for feature, correlations in su_values.items():\n",
        "        for correlation in correlations:\n",
        "            other_feature, su_value = correlation\n",
        "            tosort.append((feature, other_feature, su_value))\n",
        "\n",
        "#     unique_tosort = set(tosort)\n",
        "#     print(len(unique_tosort))\n",
        "    sorted_unique_tosort = sorted(tosort, key=lambda x: abs(x[2]))\n",
        "    print('Overlapping of features')\n",
        "    print(sorted_unique_tosort)\n",
        "\n",
        "    index_map = {feature: i for i, (feature, _) in enumerate(sorted_su_values)}\n",
        "\n",
        "    features_to_remove = []\n",
        "\n",
        "    for i in range(-3, 0):\n",
        "        f1, f2, _ = sorted_unique_tosort[i]\n",
        "        if f1 in [item[0] for item in sorted_su_values] and f2 in [item[0] for item in sorted_su_values]:\n",
        "            f1_index = index_map[f1]\n",
        "            f2_index = index_map[f2]\n",
        "            feature_to_remove = f1 if f1_index > f2_index else f2\n",
        "#             print(feature_to_remove)\n",
        "            features_to_remove.append(feature_to_remove)\n",
        "\n",
        "\n",
        "    if features_to_remove:\n",
        "        max_index = max(index_map[feature] for feature in features_to_remove)\n",
        "        feature_to_remove = next(feature for feature, index in index_map.items() if index == max_index)\n",
        "        data.drop(columns=[feature_to_remove], inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "    selected_columns.extend(data.columns)\n",
        "    return data, selected_columns\n",
        "\n",
        "\n",
        "file_path = '/content/sample_data/smoteagain.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "target_column = 'num'\n",
        "# print(data.columns)\n",
        "updated_data, selected_columns = feature_selection_by_su(data, target_column)\n",
        "new_file_path = '/content/sample_data/fcbfdata.csv'\n",
        "updated_data.to_csv(new_file_path, index=False)\n",
        "\n",
        "print(\"Columns selected by SU feature selection method:\")\n",
        "print(selected_columns)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "de62272f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de62272f",
        "outputId": "269701ce-c786-4cc8-ce75-968a0f81eaa3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9637681159420289\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "file_path = '/content/sample_data/fcbfdata.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "target_column = \"num\"\n",
        "\n",
        "X = data.drop(columns=[target_column])\n",
        "y = data[target_column]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "et_classifier = ExtraTreesClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "\n",
        "et_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = et_classifier.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78ceb810",
      "metadata": {
        "id": "78ceb810",
        "outputId": "1576cb44-cb36-4365-948c-1f6a2be2948a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          age  sex        cp  trestbps      chol  fbs  restecg    thalch  \\\n",
            "67   0.520833    1  0.666667  0.528302  0.284483    0  0.00000  0.717557   \n",
            "137  0.125000    1  0.000000  0.245283  0.211207    0  0.50000  0.450382   \n",
            "461  0.504182    0  0.000000  0.599141  0.467896    0  0.77509  0.436989   \n",
            "578  0.452121    1  0.000000  0.150943  0.208647    0  0.00000  0.293233   \n",
            "17   0.520833    1  0.000000  0.433962  0.299569    0  0.50000  0.679389   \n",
            "..        ...  ...       ...       ...       ...  ...      ...       ...   \n",
            "760  0.748302    0  0.000000  0.528302  0.285379    0  0.00000  0.340686   \n",
            "765  0.614863    1  0.000000  0.501108  0.252101    0  0.00000  0.475486   \n",
            "751  0.593214    1  0.000000  0.482587  0.318808    0  0.00000  0.515777   \n",
            "626  0.484631    1  0.000000  0.314878  0.373222    0  0.50000  0.402161   \n",
            "160  1.000000    1  0.000000  0.292453  0.439655    0  0.00000  0.694656   \n",
            "\n",
            "     exang   oldpeak  slope        ca      thal  num  \n",
            "67       0  0.258065    1.0  0.000000  1.000000    0  \n",
            "137      1  0.258065    0.5  0.000000  1.000000    1  \n",
            "461      1  0.388773    0.5  0.000000  0.724910    2  \n",
            "578      1  0.075995    0.5  0.254805  0.617792    3  \n",
            "17       0  0.193548    1.0  0.000000  0.500000    0  \n",
            "..     ...       ...    ...       ...       ...  ...  \n",
            "760      0  0.181007    0.5  1.000000  1.000000    4  \n",
            "765      1  0.393483    0.5  0.333333  0.707215    4  \n",
            "751      1  0.256738    0.5  0.498286  1.000000    4  \n",
            "626      1  0.582408    0.5  0.719490  1.000000    3  \n",
            "160      1  0.000000    1.0  1.000000  0.500000    4  \n",
            "\n",
            "[800 rows x 14 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\heart_disease_uci.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "columns_with_missing_values = ['trestbps', 'chol', 'thalch', 'oldpeak','fbs', 'restecg', 'ca', 'slope', 'thal']\n",
        "# Drop missing values from specified columns\n",
        "df.dropna(subset=columns_with_missing_values, inplace=True)\n",
        "# Encoding string to numerical\n",
        "label_encoder = LabelEncoder()\n",
        "columns=['sex','cp','fbs','restecg','exang','slope','thal']\n",
        "for column in columns:\n",
        "    df[column] = label_encoder.fit_transform(df[column])\n",
        "# Normalization\n",
        "columns_tonormalize = ['age','cp','trestbps', 'chol', 'restecg', 'thalch', 'oldpeak', 'slope', 'ca', 'thal']\n",
        "scaler = MinMaxScaler()\n",
        "df[columns_tonormalize] = scaler.fit_transform(df[columns_tonormalize])\n",
        "# Outlier removal\n",
        "isolation_forest = IsolationForest(contamination=0.1, random_state=42)\n",
        "outliers = isolation_forest.fit_predict(df)\n",
        "df_filtered = df[outliers != -1]\n",
        "# SMOTE - Oversampling\n",
        "X = df.drop('num', axis=1)\n",
        "y = df['num']\n",
        "smote = SMOTE()\n",
        "X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "resampled_df = pd.DataFrame(X_resampled, columns=X.columns)\n",
        "resampled_df['num'] = y_resampled\n",
        "resampled_df = resampled_df.sample(frac=1)\n",
        "new_file_path = r'C:\\Users\\THASNEEM FATHIMA\\Downloads\\resampled_dataset.csv'\n",
        "resampled_df.to_csv(new_file_path, index=False)\n",
        "print(resampled_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b49b0d7",
      "metadata": {
        "id": "3b49b0d7",
        "outputId": "889c0d42-f951-4bc3-e2d5-d5c1f518b3d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: imbalanced-learn in c:\\users\\thasneem.desktop-met98bf\\anaconda3\\lib\\site-packages (0.8.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\thasneem.desktop-met98bf\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.24.3)\n",
            "Requirement already satisfied: scipy>=0.19.1 in c:\\users\\thasneem.desktop-met98bf\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.11.1)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in c:\\users\\thasneem.desktop-met98bf\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.4.1.post1)\n",
            "Requirement already satisfied: joblib>=0.11 in c:\\users\\thasneem.desktop-met98bf\\anaconda3\\lib\\site-packages (from imbalanced-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\thasneem.desktop-met98bf\\anaconda3\\lib\\site-packages (from scikit-learn>=0.24->imbalanced-learn) (2.2.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install imbalanced-learn"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}